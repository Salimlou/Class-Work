Mark Randles
CS417
Lab 1 Report
2004-02-18

There were some serious problems encountered during the progress of this lab.  The largest problem by far was the cluster not allowing me to use more then 10 nodes.  I ran a series of tests to determine if it was an algorithmic error or a hardware/software error and after running a completely empty program it became quite obvious to me that it was a problem with the cluster and not my code.  The very basic MPI program would crash approx. 15s into the initlization in what i would venture to guess during the MPI_Init() call.  This program would run fine with under 10 nodes but as soon as it was run with 11 or more it would crash.

This error hampered my data gathering as I could only get data for the first 5 data points (1,3,5,7,9 nodes).  However from the singular example of one version of my code that managed to run on all 17 nodes the data retrived for 9 nodes is close enough to where the speedup curve starts to level off that we could assume that it's a very rough estimate of the max speedup possible for this program.

During the diagnosis of the problem that i believe exists with the cluster, i tried a variety of different methods to communicate between the children and parent.  The most efficient way to do it according to all sources in my reference would be to use the MPI native call to MPI_Gather().  However because of the design of the parallel structure this is inefficient as MPI_Gather() wants data from the parent which there is none.  My second approach was to try to use a single for loop with independent parts for the parent and child.  This approach was unstable unless a MPI_Barrier call was made at the end of the loop.  Lastly i used a ordered send/recieve.  Each child would block till the parent sends a single integer flag, which signals to the child that the parent is ready for recieve.  The child then unblocks and sends it data to the parent.

The last approach seems to be the quickest for data transfer.  Usually it was an improvement of between 10 to 100 milliseconds between the second way and the third and fastest way.  This still isn't the fastest method of send/recieve but for our problem it is sufficient.  Overall most of the time that the program consumed was during the processing of the matrix by the children processes.  During no run was the total communication time was over 1s.

The actual algorithim that multiplies the matricies togather isn't very interesting.  The most interesting part of the entire algorithm was what happens when the rows of matrix A cannot be divided evenly between the children.  Since the parent is sitting stagnet waiting for the children to finish processing, I developed a specialization of the algorithm that the children were using so that the parent would multiply the remaining lines of matrix A.  This is a good approach because it doesn't just leave the parent process sitting idle and it keeps the load on all of the child processes even.  However this wouldn't be the ideal solution for many problems.

The speedup of the program was nearly linear and if more runs had been done and the times adveraged it would probally come out to nearly linear.  Since the times concidered in the speedup also include the communication time we will always be just under the ideal curve but always within a acceptable range.

This problem was very easy to parallelize and provided good data to make resonable conclusions from.  There are a few optimizations that could be made, however they wouldn't change the results any.  Because of the way that matrix multiplication happens I wouldn't assume that there is an answer that is has a speedup that is greater then linear, although optimizations might allow for a speedup greater then n (where n = number of nodes).  If we had not had that strange hardware problem I would expect that the data would be more solid, but the results wouldn't change much.